    # Шаг 1: Загрузка данных и базовая подготовка

# Загрузка необходимых библиотек
library(ggplot2)
library(quantreg)
library(readxl)

# Загрузка данных
data <- read_excel("/Users/tatyana/Downloads/data_lab7.xlsx") 

    # Шаг 2: Выявление корреляции

# Проверка корреляции между общей площадью (totsp) и ценой (price)
correlation <- cor(data$totsp, data$price)
print(correlation) #результат - значимая положительная корреляция


    # Шаг 3: Разделение данных на обучающую и тестовую выборки

set.seed(1) # Фиксируем случайные числа
train_index <- sample(1:nrow(data), size = 0.7 * nrow(data))
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

    # Шаг 4: Построение модели линейной регрессии (метод наименьших квадратов) - минимизировать сумму квадратов отклонений предсказ-х зн-й от фактич-х

# Построение модели линейной регрессии
lm_model <- lm(price ~ totsp, data = train_data)
summary(lm_model)  # Отображение результатов модели

#Результаты
      #1. Коэффициенты модели:
#Интерцепт (Intercept): -16.50215 - ожидаемая цена (price), когда общая площадь (totsp) равна нулю. Несмотря на то, что этот коэффициент 
#не всегда имеет физический смысл (с точки зрения недвижимости площадь не может быть равна нулю), он помогает в расчете предсказания модели.
#Коэффициент для totsp: 1.41120. Это означает, что при увеличении общей площади на 1 квадратный метр цена квартиры увеличивается в среднем 
#на 1.41120 единиц валюты. 
#Коэффициент статистически значим, так как p-value < 2e-16.
      #2. Остатки (Residuals):
# Минимальное значение: -71.73, максимальное значение: 664.49. Остатки показывают разницу между предсказанными 
#значениями модели и реальными значениями. 
#Наличие как положительных, так и отрицательных значений указывает на то, что модель иногда недооценивает, а иногда переоценивает цену.
     #3. R-квадрат (Multiple R-squared): 0.5571.
# Это значение означает, что примерно 55.71% изменчивости зависимой переменной (цены) объясняется моделью через общую площадь. 
#Это умеренный показатель, который может быть улучшен путем включения дополнительных предикторов.
     #4. Скорректированный R-квадрат (Adjusted R-squared): 0.5569.
# Скорректированный R-квадрат учитывает количество предикторов и размер выборки. Здесь он близок к R-квадрат, что указывает на небольшое 
#количество ненужных предикторов.
     #5. F-статистика и p-value:
# F-статистика: 2837, p-value: < 2.2e-16. Это говорит о том, что модель в целом статистически значима, и по крайней мере один из предикторов 
#(в данном случае только totsp) влияет на зависимую переменную.


    # Шаг 5: Графическая оценка модели

# QQ-plot для оценки нормальности остатков
qqnorm(lm_model$residuals)
qqline(lm_model$residuals)

# График остатков для оценки гомоскедастичности
plot(lm_model$fitted.values, lm_model$residuals,
     xlab = "Предсказанные значения", ylab = "Остатки",
     main = "График остатков")
abline(h = 0, col = "red")

#Результаты
#1. Q-Q график - Этот график используется для проверки нормальности распределения остатков, если данные распределены нормально, 
#точки должны располагаться вдоль прямой линии.
# В данном случае, наблюдается значительное отклонение от линии. Особенно на обоих концах графика (в хвостах) точки расположены 
#далеко от линии, что указывает на то, что остатки не распределены нормально. Это может свидетельствовать о наличии выбросов 
#или нелинейных зависимостях.

#2. График остатков - Этот график позволяет оценить однородность дисперсии остатков (гетероскедастичность) и наличие зависимости 
#между предсказанными значениями и остатками.Если остатки распределены случайным образом вокруг горизонтальной линии (0), 
#это говорит о нормальном распределении и равномерной дисперсии. 
#В данном случае точки сосредоточены близко к нулю, но возможны небольшие отклонения, что может указывать на 
#присутствие гетероскедастичности или структурированных остатков.


    # Шаг 6: Вычисление метрик качества

# Вычисление ошибок для обучающей выборки
train_predictions <- predict(lm_model, train_data)
train_MSE <- mean((train_data$price - train_predictions)^2)
train_RMSE <- sqrt(train_MSE)
train_R2 <- summary(lm_model)$r.squared

# Вычисление ошибок для тестовой выборки
test_predictions <- predict(lm_model, test_data)
test_MSE <- mean((test_data$price - test_predictions)^2)
test_RMSE <- sqrt(test_MSE)
test_R2 <- 1 - (sum((test_data$price - test_predictions)^2) / sum((test_data$price - mean(test_data$price))^2))

train_MSE; train_RMSE; train_R2
test_MSE; test_RMSE; test_R2

# Результаты
# 1. Среднеквадратичная ошибка (MSE): Обучающая выборка (train_MSE): 586.8348. Тестовая выборка (test_MSE): 406.8753.
# MSE измеряет среднюю величину квадратов ошибок между предсказанными и фактическими значениями. Меньшие значения указывают 
#на то, что модель лучше адаптирована к данным.

# 2. Корень из среднеквадратичной ошибки (RMSE):Обучающая выборка (train_RMSE): 24.22467. Тестовая выборка (test_RMSE): 20.17115.
# RMSE является более интуитивной метрикой, так как он выражается в тех же единицах, что и переменная (в данном случае цена). 
#RMSE должен быть как можно меньше, и здесь мы видим, что тестовая выборка имеет меньший RMSE, что является положительным признаком.

#3. Коэффициент детерминации (R^2): Обучающая выборка (train_R2): 0.5571. Тестовая выборка (test_R2): 0.6251.
# Значение R^2 показывает, какая доля изменчивости зависимой переменной будет объяснена моделью. Значение 0.5571 для 
#обучающей выборки и 0.6251 для тестовой выборки указывает на то, что модель более эффективно объясняет вариацию данных в 
#тестовой выборке, что может быть связано с тем, что модель фокусируется на более репрезентативной выборке.

#Значения RMSE и R^2 показывают, что модель имеет умеренно хорошую предсказательную способность как для обучающей, так и для тестовой выборки
#Разница в R^2 и RMSE (среднеквадратичес.откл.) между обучающей и тестовой выборками может указывать на то, что модель хорошо обобщает данные, 
#и не наблюдается значительной переобученности.


    # Модель квантильной регрессии - минимизировать сумму абсолютных отклонений 
    # Шаг 7: Построение моделей квантильной регрессии

# Модели для различных квантилей
quantiles <- c(0.1, 0.5, 0.9)
quantile_models <- lapply(quantiles, function(q) rq(price ~ totsp, data = train_data, tau = q))

    # Шаг 8: Вычисление ошибок для квантильной регрессии

weighted_MAE <- function(predictions, actual, weights) {
  sum(weights * abs(predictions - actual)) / sum(weights)
}

weighted_MAPE <- function(predictions, actual, weights) {
  sum(weights * abs((predictions - actual) / actual)) / sum(weights) * 100
}

# Выбрать веса
weights <- rep(1, nrow(test_data)) # пример просто одинаковых весов 

# Вычисление ошибок для каждой квантильной регрессии
quantile_errors <- sapply(quantile_models, function(model) {
  quantile_predictions <- predict(model, test_data)
  c(
    weighted_MAE(quantile_predictions, test_data$price, weights),
    weighted_MAPE(quantile_predictions, test_data$price, weights)
  )
})

        #Выводы по матрице quantile_errors
    # 1. Сравнение значений по строкам:
#Weighted MAE (средняя абсолютная ошибка): Значения показывают, насколько хорошо предсказания модели подходят к фактическим значениям. 
# Для квантиля 0.1 (14.99) и квантиля 0.5 (10.32) ошибки меньше, чем для квантиля 0.9 (18.97). Это говорит о том, что модель 
#лучше предсказывает объекты с низкими и средними ценами, но хуже справляется с высокими ценами (квартирами с высокой ценой).

#Weighted MAPE (средняя абсолютная процентная ошибка):
#Для квантиля 0.1 (21.43), 0.5 (15.55) и 0.9 (36.11) показывает процентные отклонения.
#Здесь видно, что процентная ошибка для 0.9 значительно выше, указывая на то, что модель дает более сильные относительные ошибки при 
#прогнозировании высоких цен. Это может указывать на проблемы в обучении модели на высоком ценовом сегменте, что требует дополнительного внимания.

    # 2. Сравнение значений по столбцам:
#Квантиль 0.1:Ошибка (MAE) 14.99 и MAPE 21.43 показывают, что модель может довольно неплохо предсказывать низкие цены, хотя ошибки всё же присутствуют. 

#Квантиль 0.5:Ошибка (MAE) 10.32 и MAPE 15.55 показывают, что модель наиболее эффективна в предсказаниях для средней ценовой категории. 
#Это является хорошим знаком, так как основной интерес может быть сосредоточен именно на среднем классе.

#Квантиль 0.9: #Самые высокие значения: MAE 18.97 и MAPE 36.11. Это свидетельствует о необходимости улучшить модель в части прогнозирования для 
#высоких ценовых диапазонов.
#Такие ошибки могут быть вызваны рядом факторов, таких как недостаток данных по высокоценным квартирам или отсутствие важной информации о таких объектах.

    # Шаг 9: Визуализация всех моделей

# Визуализация
# Создаем основной график с точками данных
plot <- ggplot(data = train_data, aes(x = totsp, y = price)) +
  geom_point(alpha = 0.5) +  # Добавляем точки данных
  
  # Линия линейной регрессии
  geom_smooth(method = "lm", color = "blue", se = FALSE, linewidth = 1) +  # Линейная модель с linewidth
  
  # Квантильная регрессия
  geom_line(data = {
    quantiles <- c(0.1, 0.5, 0.9)
    lines_data <- data.frame()  # Создаем пустой дата-фрейм для хранения линий
    
    for (q in quantiles) {
      # Создаем модель квантильной регрессии для текущего уровня q
      model <- rq(price ~ totsp, data = train_data, tau = q)
      
      # Создаем новые данные для предсказания
      new_data <- data.frame(totsp = seq(min(train_data$totsp), max(train_data$totsp), length.out = 100))
      
      # Получаем предсказания на основе модели
      predicted <- predict(model, new_data)
      
      # Объединяем результаты в один дата-фрейм
      lines_data <- rbind(lines_data, cbind(new_data, predicted, quantile = q))  # Добавляем данные
    }
    lines_data
  },
  aes(x = totsp, y = predicted, color = factor(quantile))) +  # Указываем цвет по фактору quantile
  scale_color_manual(values = c("red", "green", "orange")) +  # Задаем цвета для квантилей
  labs(title = "Regression Models: Data Points and Quantile Regression Lines",
       x = "Общая площадь (totsp)",
       y = "Цена (рrice)",
       color = "Quantile") +  # Обозначаем легенду для квантили
  theme_minimal()

# Отображаем график
print(plot)

    #На графике отображены:
#1. Точки данных: Темно-серые точки представляют собой наблюдения. Они показывают разброс цен в зависимости от общего пространства. 

#2. Квантильные регрессионные линии:
# Красная линия (квантиль 0.1): Находится ниже других линий, что указывает на то, 
#что 10% самых низких цен наблюдаются при меньшем объеме пространства.
# Зеленая линия (квантиль 0.5): Это медиана, представляющая собой центральное значение, 
#указывающее, что 50% наблюдений находятся ниже этой линии.
# Синяя линия (квантиль 0.9): Находится выше остальных линий, что означает, 
#что для 90% наблюдений цены выше этой линии.

      # Анализ:
# Тенденция: Все линии восходящие, что говорит о положительной корреляции между общим пространством и ценой. 
#С увеличением пространства цены также растут.

# Разброс данных: Наблюдается широкий разброс цен при относительно маленьком пространства. 
#Это может указывать на наличие других факторов, влияющих на цену недвижимости.

# Преобладание: Разница между квантилями показывает, что для более дорогих объектов (90-й процентиль) 
#увеличивается не только цена, но и пространство, а для более дешевых объектов (10-й процентиль) 
#цена значительно ниже даже при небольшом пространстве.

